{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import chain\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.function import Function\n",
    "import torch.nn.utils.prune as prune\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn, einsum\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "import matplotlib as mp\n",
    "import time\n",
    "from scipy.signal import savgol_filter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, silhouette_score, silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import data_handler\n",
    "from utils import benchmark\n",
    "from utils import visualisation\n",
    "from utils.models.VQ_VAE import Model\n",
    "import helpers\n",
    "\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I'm plugging my own dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_genes_file_1 = '../../data/PD_genes_of_interest/signif_de_Disease_status_IDIOPATHIC_PD_01.tsv'\n",
    "suggested_genes_file_2 = '../../data/PD_genes_of_interest/signif_de_PD_DIAGNOSIS_0.1.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pay attention to executing the right cell for the desired experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 1\n",
    "x_train, filenames, n_genes, gene_names = data_handler.generate_dataset_transcripts(\n",
    "    feature_selection_threshold = 5, \n",
    "    gene_selection_file = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 2\n",
    "x_train, filenames, n_genes, gene_names = data_handler.generate_dataset_transcripts(\n",
    "    feature_selection_threshold = 0, \n",
    "    gene_selection_file = suggested_genes_file_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment 3\n",
    "x_train, filenames, n_genes, gene_names = data_handler.generate_dataset_transcripts(\n",
    "    feature_selection_threshold = 0, \n",
    "    gene_selection_file = suggested_genes_file_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of experiment choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate(list(x_train.as_numpy_iterator()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding my own labels :\n",
    "\n",
    "metadata_path = \"../../METADATA_200123.xlsx\"\n",
    "\n",
    "meta_data = pd.read_excel(metadata_path, header = 1, usecols = range(1,10) )\n",
    "patient_ids = [int(name.split(\".\")[1]) for name in filenames]\n",
    "\n",
    "# Create a dictionary to map 'Patient Number' to 'Disease Status'\n",
    "patient_disease_status = dict(zip(meta_data['Patient Number'], meta_data['Disease Status']))\n",
    "\n",
    "label = [patient_disease_status.get(patient_id, None) for patient_id in patient_ids]\n",
    "label = pd.Series(label)\n",
    "num_classes = len(label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to the regular notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame(data)\n",
    "feature_num = dat.shape[1]\n",
    "data_num = dat.shape[0]\n",
    "dat = dat.values.reshape(-1,1,feature_num)\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data1 ,transform = None):\n",
    "        self.transform = transform\n",
    "        self.data1 = data1\n",
    "        self.datanum = len(data1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        out_data1 = torch.tensor(self.data1[idx]).float() \n",
    "        if self.transform:\n",
    "            out_data1 = self.transform(out_data1)\n",
    "\n",
    "        return out_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dat, test_size = 0.3,random_state = 66)\n",
    "print('train data:',len(train_data))\n",
    "print('test data:',len(test_data))\n",
    "train_data_set = Mydatasets(data1 = train_data)\n",
    "test_data_set = Mydatasets(data1 = test_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data_set, batch_size = 256, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_set, batch_size = 32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for m1 mac\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "out_dim = 512   \n",
    "VQ_VAE = Model(\n",
    "            dropout = 0.0,\n",
    "            input_size = feature_num, \n",
    "            encoder_dim = out_dim,\n",
    "            num_embeddings = 64,  \n",
    "            embedding_dim = 64,   \n",
    "            commitment_cost = 1\n",
    "           ).to(DEVICE)\n",
    "\n",
    "Classifier_loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(VQ_VAE.parameters(), lr=1e-3, amsgrad=False)\n",
    "data_variance = np.var(dat)\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added feature to monitor training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1000\n",
    "train_res_recon_error = []\n",
    "val_res_recon_error = []\n",
    "\n",
    "VQ_VAE.train()\n",
    "\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    quantized_merge = torch.empty(0, 1, 64).to(DEVICE)\n",
    "    \n",
    "    # Training loop\n",
    "    for labels, inputs in enumerate(train_dataloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        vq_loss, data_recon, perplexity, _, quantized = VQ_VAE(inputs)\n",
    "        recon_error = F.mse_loss(data_recon, inputs) / data_variance\n",
    "        loss = recon_error + vq_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate and store training loss for this epoch\n",
    "    train_loss = running_loss / count\n",
    "    train_res_recon_error.append(train_loss)\n",
    "    \n",
    "    # Validation loop (if you have a validation dataset)\n",
    "    with torch.no_grad():\n",
    "        running_val_loss = 0.0\n",
    "        count = 0\n",
    "        for labels, inputs in enumerate(test_dataloader, 0):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            vq_loss, data_recon, perplexity, _, quantized = VQ_VAE(inputs)\n",
    "            recon_error = F.mse_loss(data_recon, inputs) / data_variance\n",
    "            loss = recon_error + vq_loss\n",
    "            count += 1\n",
    "            running_val_loss += loss.item()\n",
    "        \n",
    "        # Calculate and store validation loss for this epoch\n",
    "        val_loss = running_val_loss / count\n",
    "        val_res_recon_error.append(val_loss)\n",
    "    \n",
    "\n",
    "    \n",
    "# Plot training and validation loss curves\n",
    "epochs = np.arange(1, EPOCH + 1)\n",
    "plt.plot(epochs, train_res_recon_error, label='Training Loss')\n",
    "plt.plot(epochs, val_res_recon_error, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lat = []\n",
    "en_quantized = []\n",
    "en_reconstruction = []\n",
    "data_set = Mydatasets(data1 = dat)\n",
    "data_set = torch.utils.data.DataLoader(data_set, batch_size = 256, shuffle=False) \n",
    "\n",
    "\n",
    "\n",
    "#vq_loss, data_recon, perplexity, _, quantized = VQ_VAE(inputs)\n",
    "\n",
    "\n",
    "for i in range(len(dat)):\n",
    "    en_data = data_set.dataset[i][0]\n",
    "    latent_1 = VQ_VAE._encoder(en_data.view(1, 1, feature_num).float().to(DEVICE))\n",
    "    _, data_recon, _, _,latent_2 = VQ_VAE(en_data.view(1, 1, feature_num).float().to(DEVICE))\n",
    "    en_lat.append(latent_1.cpu().detach().numpy())\n",
    "    en_quantized.append(latent_2.cpu().detach().numpy())\n",
    "    en_reconstruction.append(data_recon.cpu().detach().numpy())\n",
    "\n",
    "encode_out = np.array(en_lat)\n",
    "encode_out = encode_out.reshape(len(dat), -1)\n",
    "quantized_out = np.array(en_quantized)\n",
    "quantized_out = quantized_out.reshape(len(dat), -1)\n",
    "reconstruction_out = np.array(en_quantized)\n",
    "reconstruction_out = reconstruction_out.reshape(len(dat), -1)\n",
    "\n",
    "print('encode_out:', encode_out.shape)\n",
    "print('quantized_out:', quantized_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatibility between notebooks\n",
    "compressed_dataframe = encode_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, analyse all groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(visualisation)\n",
    "importlib.reload(helpers)\n",
    "\n",
    "print(\"######################## OG Groups : \")\n",
    "TSNE_params = {\n",
    "            \"early_exaggeration\" : 50,\n",
    "            \"learning_rate\" : 500, \n",
    "            #\"perplexity\" : 75, \n",
    "            \"perplexity\" : 100, \n",
    "            \"min_grad_norm\" : 1e-7, \n",
    "            \"n_iter\" : 2000,\n",
    "            \"n_components\" : 2\n",
    "        }\n",
    "visualisation.plot_clusters(compressed_dataframe, label, TSNE_params)\n",
    "benchmark.print_metrics(compressed_dataframe, label)\n",
    "\n",
    "print(\"######################## KNMEANS : \")\n",
    "kmeans_labels = helpers.auto_kMean(compressed_dataframe)\n",
    "visualisation.plot_clusters(compressed_dataframe, kmeans_labels, TSNE_params)\n",
    "benchmark.print_metrics(compressed_dataframe, kmeans_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second, Check if control group leads to any clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering data\n",
    "clinical_info_series = pd.Series(label)\n",
    "filenames_series = pd.Series(filenames)\n",
    "\n",
    "# Create boolean masks for filtering\n",
    "clinical_mask = clinical_info_series == \"Healthy Control\"\n",
    "\n",
    "\n",
    "# Apply the mask to your dataset\n",
    "filtered_dat = compressed_dataframe[clinical_mask]\n",
    "filtered_clinical_info = clinical_info_series[clinical_mask]\n",
    "\n",
    "# Check the shape of the filtered dataset\n",
    "print(filtered_dat.shape)\n",
    "print(filtered_clinical_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(visualisation)\n",
    "importlib.reload(helpers)\n",
    "\n",
    "print(\"######################## OG Groups : \")\n",
    "TSNE_params = {\n",
    "            \"early_exaggeration\" : 50,\n",
    "            \"learning_rate\" : 500, \n",
    "            \"perplexity\" : 50, \n",
    "            \"min_grad_norm\" : 1e-7, \n",
    "            \"n_iter\" : 2000,\n",
    "            \"n_components\" : 2\n",
    "        }\n",
    "visualisation.plot_clusters(filtered_dat, filtered_clinical_info, TSNE_params)\n",
    "\n",
    "print(\"######################## KNMEANS : \")\n",
    "kmeans_labels = helpers.auto_kMean(filtered_dat)\n",
    "visualisation.plot_clusters(filtered_dat, kmeans_labels, TSNE_params)\n",
    "benchmark.print_metrics(filtered_dat, kmeans_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third : check for clusters in the Idiopathic PD group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering data\n",
    "# Convert clinical_info and filenames to Series for easy filtering\n",
    "clinical_info_series = pd.Series(label)\n",
    "filenames_series = pd.Series(filenames)\n",
    "\n",
    "# Create boolean masks for filtering\n",
    "clinical_mask = clinical_info_series == \"Idiopathic PD\"\n",
    "\n",
    "\n",
    "# Apply the mask to your dataset\n",
    "filtered_dat = compressed_dataframe[clinical_mask]\n",
    "filtered_clinical_info = clinical_info_series[clinical_mask]\n",
    "\n",
    "# Check the shape of the filtered dataset\n",
    "print(filtered_dat.shape)\n",
    "print(filtered_clinical_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(visualisation)\n",
    "importlib.reload(helpers)\n",
    "\n",
    "print(\"######################## OG Groups : \")\n",
    "params = {\"early_exaggeration\":10,\n",
    "              \"learning_rate\":700, \n",
    "              \"perplexity\":15, \n",
    "              \"min_grad_norm\":1e-7, \n",
    "              \"n_iter\":1000,\n",
    "              \"n_components\":2}\n",
    "visualisation.plot_clusters(filtered_dat, filtered_clinical_info, TSNE_params)\n",
    "\n",
    "print(\"######################## KNMEANS : \")\n",
    "kmeans_labels = helpers.auto_kMean(filtered_dat)\n",
    "visualisation.plot_clusters(filtered_dat, kmeans_labels, TSNE_params)\n",
    "benchmark.print_metrics(filtered_dat, kmeans_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last : Check for Genetic PD groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering data\n",
    "# Convert clinical_info and filenames to Series for easy filtering\n",
    "clinical_info_series = pd.Series(label)\n",
    "filenames_series = pd.Series(filenames)\n",
    "\n",
    "# Create boolean masks for filtering\n",
    "clinical_mask = clinical_info_series == \"Genetic PD\"\n",
    "\n",
    "\n",
    "# Apply the mask to your dataset\n",
    "filtered_dat = compressed_dataframe[clinical_mask]\n",
    "filtered_clinical_info = clinical_info_series[clinical_mask]\n",
    "\n",
    "# Check the shape of the filtered dataset\n",
    "print(filtered_dat.shape)\n",
    "print(filtered_clinical_info.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(visualisation)\n",
    "importlib.reload(helpers)\n",
    "\n",
    "print(\"######################## OG Groups : \")\n",
    "params = {\"early_exaggeration\":5,\n",
    "              \"learning_rate\":100, \n",
    "              \"perplexity\":30, \n",
    "              \"min_grad_norm\":1e-7, \n",
    "              \"n_iter\":5000,\n",
    "              \"n_components\":2}\n",
    "visualisation.plot_clusters(filtered_dat, filtered_clinical_info, TSNE_params)\n",
    "\n",
    "print(\"######################## KNMEANS : \")\n",
    "kmeans_labels = helpers.auto_kMean(filtered_dat)\n",
    "visualisation.plot_clusters(filtered_dat, kmeans_labels, TSNE_params)\n",
    "benchmark.print_metrics(filtered_dat, kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
