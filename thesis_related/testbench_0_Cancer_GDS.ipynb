{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "\n",
    "# project specific\n",
    "sys.path.append('../src')\n",
    "import helpers\n",
    "from utils import benchmark, data_handler, visualisation\n",
    "from models import VQ_VAE_0, VQ_VAE_1\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "\n",
    "pd.options.display.width = 1000\n",
    "\n",
    "\n",
    "absolute_path = \"c:/thesis/data/cancer\"\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import pickle\n",
    "\n",
    "# for translation of gene symbols\n",
    "import mygene\n",
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "%load_ext tensorboard\n",
    "!rm -rf ../workfiles/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../workfiles/BRCA_ds.pkl', 'rb') as f:\n",
    "#with open('../workfiles/normed_BRCA_ds.pkl', 'rb') as f:\n",
    "    data, metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = data\n",
    "print(dat.shape)\n",
    "feature_num = dat.shape[1]\n",
    "dat = dat.reshape(-1,1,feature_num)\n",
    "print(dat.shape)\n",
    "label = metadata[\"PAM50_labels\"]\n",
    "feature_num = metadata[\"n_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data1 ,transform = None):\n",
    "        self.transform = transform\n",
    "        self.data1 = data1\n",
    "        self.datanum = len(data1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        out_data1 = torch.tensor(self.data1[idx]).float() \n",
    "        if self.transform:\n",
    "            out_data1 = self.transform(out_data1)\n",
    "\n",
    "        return out_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dat, test_size = 0.1, random_state = 66)\n",
    "print('train data:',len(train_data))\n",
    "print('test data:',len(test_data))\n",
    "train_data_set = Mydatasets(data1 = train_data)\n",
    "test_data_set = Mydatasets(data1 = test_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data_set, batch_size = 32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m1 mac\n",
    "#DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(VQ_VAE_1)\n",
    "\n",
    "\n",
    "# new best performer\n",
    "out_dim = 64   \n",
    "VQ_VAE = VQ_VAE_1.Model(\n",
    "            dropout = 0.1,\n",
    "            input_size = feature_num, \n",
    "            encoder_dim = out_dim,\n",
    "            num_embeddings = 512,\n",
    "            embedding_dim = 32,   \n",
    "            commitment_cost = 1,\n",
    "            decay= 0\n",
    "           ).to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Classifier_loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(VQ_VAE.parameters(), lr=1e-4, amsgrad=False)\n",
    "data_variance = np.var(dat)\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res_recon_error = []\n",
    "perplexities = []\n",
    "frames = []\n",
    "n_frames = 0\n",
    "\n",
    "def callbacks(epoch):\n",
    "    # Code to run every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Your additional code here\n",
    "        en_lat = []\n",
    "        en_quantized = []\n",
    "        en_reconstruction = []\n",
    "\n",
    "        VQ_VAE.eval()\n",
    "\n",
    "        data_set = Mydatasets(data1 = dat)\n",
    "        data_set = torch.utils.data.DataLoader(data_set, batch_size = 256, shuffle=False) \n",
    "\n",
    "\n",
    "        for i in range(len(dat)):\n",
    "            en_data = data_set.dataset[i][0]\n",
    "            latent_1 = VQ_VAE._encoder(en_data.view(1, 1, feature_num).float().to(DEVICE))\n",
    "            _, data_recon, _, _,latent_2 = VQ_VAE(en_data.view(1, 1, feature_num).float().to(DEVICE))\n",
    "            en_lat.append(latent_1.cpu().detach().numpy())\n",
    "            en_quantized.append(latent_2.cpu().detach().numpy())\n",
    "            en_reconstruction.append(data_recon.cpu().detach().numpy())\n",
    "\n",
    "        encode_out = np.array(en_lat).reshape(len(dat), -1)\n",
    "        quantized_out = np.array(en_quantized).reshape(len(dat), -1)\n",
    "        reconstruction_out = np.array(en_reconstruction).reshape(len(dat), -1)\n",
    "\n",
    "\n",
    "        stack = np.vstack([dat[0].reshape(1, -1), reconstruction_out[0].reshape(1, -1)])\n",
    "\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(encode_out)\n",
    "        pca_result = pca.transform(encode_out)\n",
    "\n",
    "        \n",
    "\n",
    "        #index_column = np.full((pca_result.shape[0], 1), epoch + 1, dtype=int)\n",
    "        index_column = np.full((pca_result.shape[0], 1), n_frames, dtype=int)\n",
    "\n",
    "        pca_result_with_index = np.hstack((index_column, pca_result))\n",
    "\n",
    "        frames.append(pca_result_with_index)\n",
    "        n_frames += 1\n",
    "\n",
    "        if (epoch + 1) % 500 == 0:\n",
    "\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "\n",
    "            # Plot the line plot in the second subplot\n",
    "            axs[0].plot(train_res_recon_error, label='Training Loss')\n",
    "            axs[0].set_title('Training Loss Plot')\n",
    "            #axs[0].set_xticks([])\n",
    "\n",
    "            sns.heatmap(stack, ax=axs[1], cbar=False)\n",
    "            axs[1].set_title('Stacked heatmap of two samples')\n",
    "            axs[1].set_xticks([])\n",
    "            axs[1].set_yticks([])\n",
    "\n",
    "\n",
    "            sns.heatmap(encode_out, ax = axs[2], cbar=False)\n",
    "            axs[2].set_title('Heatmap of hole quantized dataset')\n",
    "            axs[2].set_xticks([])\n",
    "            axs[2].set_yticks([])\n",
    "\n",
    "\n",
    "            sns.scatterplot(x = pca_result[:, 0], y = pca_result[:, 1], c=label, ax=axs[3])\n",
    "            axs[3].set_title('PCA')\n",
    "            axs[3].set_xticks([])\n",
    "            axs[3].set_yticks([])\n",
    "\n",
    "            plt.subplots_adjust(wspace=0)  \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 5000\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', min_lr= 0.000001)\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "VQ_VAE.train()\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCH)):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    quantized_merge = torch.empty(0, 1, 64).to(DEVICE)\n",
    "    \n",
    "    # Training loop\n",
    "    for _, inputs in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        vq_loss, data_recon, perplexity, _, quantized = VQ_VAE(inputs)\n",
    "        recon_error = F.mse_loss(data_recon, inputs) / data_variance\n",
    "        loss = recon_error + vq_loss #+ perplexity\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate and store training loss for this epoch\n",
    "    train_loss = running_loss / count\n",
    "    train_res_recon_error.append(train_loss)\n",
    "    perplexities.append(perplexity.cpu().detach().numpy())\n",
    "    callbacks(epoch)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Plot training and validation loss curves\n",
    "epochs = np.arange(1, EPOCH + 1)\n",
    "plt.plot(epochs, train_res_recon_error, label='Training Loss')\n",
    "#plt.plot(epochs, val_res_recon_error, label='Validation Loss')\n",
    "plt.plot(epochs, perplexities, label='Perplexity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(optimizer.param_groups[0]['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "#%matplotlib notebook\n",
    "\n",
    "# Create a figure and axis for the animation\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Define an update function for the animation\n",
    "def update(frame):\n",
    "    ax.clear()\n",
    "    ax.set_title(f'Frame {frame}')\n",
    "    \n",
    "    # Get the PCA result for the current frame\n",
    "    pca_result = frames[frame]\n",
    "    \n",
    "    # Scatter plot of PCA results with color based on index\n",
    "    scatter = ax.scatter(pca_result[:, 1], pca_result[:, 2], c=label)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the animation\n",
    "ani = FuncAnimation(fig, update, frames=n_frames, repeat=True)\n",
    "\n",
    "\n",
    "\n",
    "# Display the animation as HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.show()\n",
    "ani.save('../img/aniPCA_GDS_VQ-VAE-1.mp4', writer='ffmpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(VQ_VAE, \"../workfiles/torch_temp\")\n",
    "torch.save(VQ_VAE, \"../workfiles/torch_VQ-VAE-1-GDS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves\n",
    "epochs = np.arange(1, EPOCH + 1)\n",
    "plt.plot(train_res_recon_error, label='Training Loss')\n",
    "plt.plot(perplexities, label='Perplexity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(optimizer.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 50\n",
    "plt.plot(train_res_recon_error[index:], label='Training Loss')\n",
    "#plt.plot(epochs[index:], val_res_recon_error[index:], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_res_recon_error[40:], label='Training Loss')\n",
    "#plt.plot(val_res_recon_error[40:], label='Training Loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lat = []\n",
    "en_quantized = []\n",
    "en_reconstruction = []\n",
    "\n",
    "VQ_VAE.eval()\n",
    "\n",
    "data_set = Mydatasets(data1 = dat)\n",
    "data_set = torch.utils.data.DataLoader(data_set, batch_size = 256, shuffle=False) \n",
    "\n",
    "\n",
    "for i in range(len(dat)):\n",
    "    en_data = data_set.dataset[i][0]\n",
    "    latent_1 = VQ_VAE._encoder(en_data.view(1, 1, feature_num).float().to(DEVICE))\n",
    "    _, data_recon, _, _,latent_2 = VQ_VAE(en_data.view(1, 1, feature_num).float().to(DEVICE))\n",
    "    en_lat.append(latent_1.cpu().detach().numpy())\n",
    "    en_quantized.append(latent_2.cpu().detach().numpy())\n",
    "    en_reconstruction.append(data_recon.cpu().detach().numpy())\n",
    "\n",
    "encode_out = np.array(en_lat)\n",
    "encode_out = encode_out.reshape(len(dat), -1)\n",
    "quantized_out = np.array(en_quantized)\n",
    "quantized_out = quantized_out.reshape(len(dat), -1)\n",
    "reconstruction_out = np.array(en_reconstruction)\n",
    "reconstruction_out = reconstruction_out.reshape(len(dat), -1)\n",
    "\n",
    "print('encode_out:', encode_out.shape)\n",
    "print('quantized_out:', quantized_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatibility between notebooks\n",
    "decoded_data = reconstruction_out\n",
    "label = pd.Series(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(encode_out.shape)\n",
    "print(quantized_out.shape)\n",
    "print(reconstruction_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sp.make_subplots(rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.1)\n",
    "# Add the original image as a heatmap-like plot\n",
    "heatmap_trace1 = go.Heatmap(z=data[0].reshape(1, -1) )\n",
    "fig.add_trace(heatmap_trace1, row=1, col=1)\n",
    "\n",
    "\n",
    "\n",
    "# Add the decoded image as a heatmap-like plot\n",
    "heatmap_trace3 = go.Heatmap(z=reconstruction_out[0].reshape(1, -1))\n",
    "fig.add_trace(heatmap_trace3, row=2, col=1)\n",
    "# Update layout\n",
    "fig.update_layout(title='Stacked Graph of Image and Latent Space', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(data, col_cluster= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(quantized_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(encode_out, col_cluster= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.clustermap(decoded_data, col_cluster= False)\n",
    "sns.heatmap(decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(visualisation)\n",
    "\n",
    "print(\"######################## OG Groups : \")\n",
    "visualisation.plot_clusters(encode_out, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what happens when i use keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import vanilla_autoencoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../workfiles/BRCA_ds.pkl', 'rb') as f:\n",
    "#with open('../workfiles/normed_BRCA_ds.pkl', 'rb') as f:\n",
    "    \n",
    "    data_original, metadata = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_names = metadata[\"sequence_names\"]\n",
    "n_genes = metadata[\"n_features\"]\n",
    "gene_names = metadata[\"feature_names\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(vanilla_autoencoder) # to allow modification of the script without restarting the whole session\n",
    "\n",
    "latent_dim = 16\n",
    "\n",
    "t_shape = (n_genes)\n",
    "\n",
    "\n",
    "autoencoder = vanilla_autoencoder.generate_model(t_shape, latent_dim)\n",
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '../workfiles/simple_ae/checkpoint'\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2,\n",
    "                              patience=20, min_lr=0.00001)\n",
    "\n",
    "early_stopping_callback = callbacks.EarlyStopping(monitor='loss', patience=70)\n",
    "\n",
    "\n",
    "log_dir = \"../workfiles/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "cb = [model_checkpoint_callback, reduce_lr, early_stopping_callback, tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = autoencoder.fit(data_original, epochs=200, callbacks=cb)  \n",
    "autoencoder.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_dataframe = autoencoder.encoder.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(compressed_dataframe, col_cluster= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_data = autoencoder.decoder.predict(compressed_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(recon_data, col_cluster= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# who gets the best loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset\n",
    "squared_error = np.square(filtered_data - reconstruction_out)\n",
    "mse = np.mean(squared_error)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow\n",
    "squared_error = np.square(data_original - recon_data)\n",
    "mse = np.mean(squared_error)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation.plot_clusters(compressed_dataframe, metadata['PAM50_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering perf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(encode_out, 'ward', 'euclidean')  # You can use other linkage methods as well\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Determine the number of clusters (adjust the threshold as needed)\n",
    "threshold = 50  # Adjust this threshold to identify clusters\n",
    "cluster_labels = fcluster(linked, threshold, criterion='distance')\n",
    "\n",
    "cm = confusion_matrix(cluster_labels, filtered_labs)\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[\"Actual 0\", \"Actual 1\", \"Actual 2\", \"Actual 3\", \"Actual 4\"], \n",
    "                     columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\", \"Predicted 3\", \"Predicted 4\"])\n",
    "\n",
    "print(cm_df)\n",
    "sns.heatmap(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linked = linkage(compressed_dataframe, 'ward', 'euclidean')  # You can use other linkage methods as well\n",
    "plt.figure(figsize=(10, 5))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Determine the number of clusters (adjust the threshold as needed)\n",
    "threshold = 290  # Adjust this threshold to identify clusters\n",
    "cluster_labels = fcluster(linked, threshold, criterion='distance')\n",
    "\n",
    "\n",
    "# Identify potential outliers (clusters with a small number of points)\n",
    "unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "outlier_clusters = unique_labels[counts < threshold]\n",
    "print(unique_labels)\n",
    "\n",
    "cm = confusion_matrix(cluster_labels, metadata['PAM50_labels'])\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[\"Actual 0\", \"Actual 1\", \"Actual 2\", \"Actual 3\", \"Actual 4\"], \n",
    "                     columns=[\"Predicted 0\", \"Predicted 1\", \"Predicted 2\", \"Predicted 3\", \"Predicted 4\"])\n",
    "\n",
    "print(cm_df)\n",
    "sns.heatmap(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
